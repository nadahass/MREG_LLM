{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m llama_cpp.server\n",
    "#!pip install pydantic_settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is working 1,2,3\n",
    "#1\n",
    "from llama_cpp import Llama \n",
    "from llama_cpp.llama_speculative import LlamaPromptLookupDecoding\n",
    "# model = Llama(model_path=\"LoraWeights_7b_4_small.gguf\", n_ctx=2048)\n",
    "model = Llama(\n",
    "    model_path=\"mreg-13b-32-hf-v2.gguf\",\n",
    "#     temperature=0.75,\n",
    "    max_tokens=2000,\n",
    "#     top_p=1,\n",
    "    n_gpu_layers= 100,\n",
    "    n_batch= 1024,\n",
    "    n_ctx=2048,\n",
    "# #     top_k=10,\n",
    "# #     top_p = 1,\n",
    "#     temperature = 0.75,\n",
    "#     cache=True,\n",
    "#     type_k=16,\n",
    "#     type_v=16,\n",
    "#     n_threads = 16,\n",
    "#     repeat_last_n = 64,\n",
    "#     repeat_penalty = 1.100000,\n",
    "#     presence_penalty = 0.000000,\n",
    "#     frequency_penalty = 0.000000,\n",
    "#     top_k = 40, \n",
    "#     tfs_z = 1.000000,\n",
    "#     top_p = 0.950000,\n",
    "#     typical_p = 1.000000,\n",
    "#     temp = 0.800000,\n",
    "#     mirostat = 0,\n",
    "#     mirostat_lr = 0.100000,\n",
    "#     mirostat_ent = 5.000000,\n",
    "#     draft_model=LlamaPromptLookupDecoding(num_pred_tokens=10) #speculative decoding \n",
    "    \n",
    ")\n",
    "\n",
    "# system_info: n_threads = 16 / 24 | AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |\n",
    "# sampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\n",
    "# generate: n_ctx = 512, n_batch = 512, n_predict = 400, n_keep = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2\n",
    "PROMPT_TEMPLATE=f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501\n",
    "### Instruction:\n",
    "[instruction]\n",
    "### Input:\n",
    "[input]\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "def create_prompt(instruction: str, inputs:str) -> str:\n",
    "    return PROMPT_TEMPLATE.replace(\"[instruction]\", instruction).replace(\"[input]\",inputs)\n",
    "\n",
    "prompt =create_prompt(\"Generate a referring expression for an object.\",\"PinkBlock2 , (0.295275000: 1.224503000: 0.150793400) , None , ( 0.295275: 1.149781: 0.1507934) ) , under & support & touching(Table,PinkBlock2) and behind(RedBlock2,PinkBlock1) and behind(RedBlock2,cup) and in_front(BlueBlock2,PinkBlock2) and behind(GreenBlock2,GreenBlock1) and under & touching & support(Table,YellowBlock1) and behind(plate,BlueBlock1) and touching(YellowBlock2,Table) and hold(Diana2,PinkBlock2) and in_front(PinkBlock1,cup) and under(Table,RedBlock1) and under(Floor,BlueBlock2) and under(Floor,cup) and in_front(BlueBlock1,plate) and under(Floor,RedBlock2) and left & touching(BlueBlock2,BlueBlock1) and under & touching & support(Table,GreenBlock2) and under & touching & support(Table,plate) and under(Floor,BlueBlock1) and behind(YellowBlock2,BlueBlock2) and in_front(YellowBlock1,plate) and right(GreenBlock2,YellowBlock2) and under & touching & support(Table,BlueBlock1) and in_front(PinkBlock2,YellowBlock2) and under & support & touching(Table,RedBlock2) and left(BlueBlock1,RedBlock1) and support(BlueBlock2,RedBlock2) and behind(cup,PinkBlock1) and behind(plate,YellowBlock1) and under(Floor,YellowBlock1) and under(Floor,YellowBlock2) and in_front(GreenBlock1,GreenBlock2) and under(Floor,GreenBlock1) and in_front(plate,YellowBlock2) and under & touching & support(Table,YellowBlock2) and under & support & touching(Table,BlueBlock2) and under(Table,GreenBlock1) and in_front(BlueBlock2,plate) and left(plate,cup) and support(RedBlock2,PinkBlock2) and touching(GreenBlock2,Table) and left(YellowBlock2,GreenBlock2) and contain(plate,PinkBlock2) and behind(YellowBlock2,YellowBlock1) and touching(RedBlock2,Table) and under(Floor,PinkBlock2) and right(GreenBlock1,BlueBlock2) and hold(Diana2,RedBlock2) and touching(Table,Floor) and touching(BlueBlock2,Table) and under(Floor,RedBlock1) and left(BlueBlock2,RedBlock1) and under(Floor,PinkBlock1) and behind(plate,BlueBlock2) and touching(BlueBlock1,Table) and under(Floor,plate) and under & touching & support(Table,cup) and right(cup,plate) and behind(YellowBlock2,plate) and in_front(PinkBlock1,RedBlock2) and under(Floor,GreenBlock2) and support(BlueBlock1,PinkBlock2) and left(YellowBlock2,RedBlock2) and right(RedBlock2,YellowBlock2) and right(GreenBlock1,BlueBlock1) and right(RedBlock1,BlueBlock2) and in_front(cup,RedBlock2) and touching(YellowBlock1,Table) and behind(PinkBlock2,BlueBlock2) and left(GreenBlock2,RedBlock2) and left(BlueBlock1,GreenBlock1) and touching(cup,Table) and right(RedBlock1,BlueBlock1) and touching(plate,Table) and support & right & touching(BlueBlock1,BlueBlock2) and in_front(BlueBlock2,YellowBlock2) and under(Table,PinkBlock1) and left(BlueBlock2,GreenBlock1) and support(GreenBlock2,BlueBlock2) and behind(YellowBlock2,PinkBlock2 , RedBlock1(-0.467860300,1.124870000,-0.376820000) and RedBlock2(0.296575000,1.125972000,0.154085400) and GreenBlock1(-0.114465600,1.124870000,-0.306613400) and GreenBlock2(-0.083701150,1.124844000,0.360018700) and BlueBlock1(0.198836000,1.124891000,-0.344644900) and BlueBlock2(0.299118700,1.125184000,-0.344177400) and PinkBlock1(-0.226616100,1.124870000,-0.162015900) and PinkBlock2(0.295275000,1.224503000,0.150793400) and YellowBlock1(0.432537600,1.124879000,-0.188665600) and YellowBlock2(0.352077900,1.124870000,0.323333000) ,  put put grasp put put put put put put grasp put put put grasp put put put put , PinkBlock2 BlueBlock1 BlueBlock1 RedBlock2 BlueBlock2 YellowBlock1 YellowBlock2 YellowBlock1 RedBlock2 RedBlock2 RedBlock2 BlueBlock2 GreenBlock2 GreenBlock2 PinkBlock2 BlueBlock2 PinkBlock2 BlueBlock1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3\n",
    "output = model(\n",
    "      prompt,\n",
    "      max_tokens=500, # Generate up to 32 tokens, set to None to generate up to the end of the context window\n",
    "#       stop=[\"### Instruction:\", \"\\n\"], # Stop generating just before the model would generate a new question\n",
    "      echo=False # Echo the prompt back in the output\n",
    ") # Generate a completion, can also call create_completion \n",
    "print(output)\n",
    "\n",
    "\n",
    "# my small model \n",
    "\n",
    "# 50 gpu layers: total time =   41933.01 ms /  1588 tokens      top_p = 0.9, temperature = 0.2,\n",
    "# 80 gpu layers: total time =   45408.25 ms /  1586 token       top_p = 0.9, temperature = 0.2,\n",
    "# 100 gpu layers: total time =  49367.65 ms /  1586 tokens      top_p = 0.9, temperature = 0.2,\n",
    "# 100 gpu layers: total time =   44118.74 ms /  1587 tokens     top_p = 0.9, temperature = 0.2,\n",
    "# 100 gpu layers: total time =   38959.39 ms /  1551 tokens     top_p = 0.9, temperature = 0.2,\n",
    "# 100 gpu layers: total time =   36817.91 ms /  1542 tokens     top_p = 1, temperature = 0.75,\n",
    "\n",
    "# my real model \n",
    "\n",
    "# gpus only \n",
    "\n",
    "# result: pointing only\n",
    "# llama_print_timings:        load time =   16264.00 ms\n",
    "# llama_print_timings:      sample time =      25.64 ms /    48 runs   (    0.53 ms per token,  1872.29 tokens per second)\n",
    "# llama_print_timings: prompt eval time =   45266.40 ms /  1541 tokens (   29.37 ms per token,    34.04 tokens per second)\n",
    "# llama_print_timings:        eval time =    9929.57 ms /    47 runs   (  211.27 ms per token,     4.73 tokens per second)\n",
    "# llama_print_timings:       total time =   55408.97 ms /  1588 tokens\n",
    "\n",
    "\n",
    "# gpus + batching 1024\n",
    "\n",
    "# llama_print_timings:        load time =   31841.73 ms\n",
    "# llama_print_timings:      sample time =      26.98 ms /    48 runs   (    0.56 ms per token,  1778.83 tokens per second)\n",
    "# llama_print_timings: prompt eval time =   49817.11 ms /  1541 tokens (   32.33 ms per token,    30.93 tokens per second)\n",
    "# llama_print_timings:        eval time =    8435.03 ms /    47 runs   (  179.47 ms per token,     5.57 tokens per second)\n",
    "# llama_print_timings:       total time =   58419.06 ms /  1588 tokens\n",
    "    \n",
    "    \n",
    "# tempreture only \n",
    "\n",
    "\n",
    "# gpus + batch 1024 + cb  (12 sec)\n",
    "\n",
    "# llama_print_timings:        load time =   36267.90 ms\n",
    "# llama_print_timings:      sample time =      27.46 ms /    48 runs   (    0.57 ms per token,  1748.00 tokens per second)\n",
    "# llama_print_timings: prompt eval time =   53748.84 ms /  1541 tokens (   34.88 ms per token,    28.67 tokens per second)\n",
    "# llama_print_timings:        eval time =    8999.93 ms /    47 runs   (  191.49 ms per token,     5.22 tokens per second)\n",
    "# llama_print_timings:       total time =   62921.77 ms /  1588 tokens\n",
    "    \n",
    "    \n",
    "# draft model + gpus layers +tempreture + cache cb (1mint:12sec)\n",
    "\n",
    "# result: pointing only\n",
    "# llama_print_timings:        load time =   16072.75 ms\n",
    "# llama_print_timings:      sample time =      22.38 ms /    48 runs   (    0.47 ms per token,  2144.87 tokens per second)\n",
    "# llama_print_timings: prompt eval time =   60025.28 ms /  1607 tokens (   37.35 ms per token,    26.77 tokens per second)\n",
    "# llama_print_timings:        eval time =    1915.14 ms /     4 runs   (  478.78 ms per token,     2.09 tokens per second)\n",
    "# llama_print_timings:       total time =   65625.05 ms /  1611 tokens\n",
    "\n",
    "# outside model\n",
    "\n",
    "# gpus only (17sec)\n",
    "\n",
    "# llama_print_timings:        load time =     177.67 ms\n",
    "# llama_print_timings:      sample time =     148.68 ms /   259 runs   (    0.57 ms per token,  1741.95 tokens per second)\n",
    "# llama_print_timings: prompt eval time =   10206.79 ms /     2 tokens ( 5103.40 ms per token,     0.20 tokens per second)\n",
    "# llama_print_timings:        eval time =   18276.22 ms /   258 runs   (   70.84 ms per token,    14.12 tokens per second)\n",
    "# llama_print_timings:       total time =   19392.82 ms /   260 tokens\n",
    "\n",
    "#  tempreture only (34 sec)\n",
    "\n",
    "# llama_print_timings:        load time =     177.41 ms\n",
    "# llama_print_timings:      sample time =      210.81 ms /   367 runs   (    0.57 ms per token,  1740.90 tokens per second)\n",
    "# llama_print_timings: prompt eval time =     177.35 ms /     6 tokens (   29.56 ms per token,    33.83 tokens per second)\n",
    "# llama_print_timings:        eval time =   25039.74 ms /   366 runs   (   68.41 ms per token,    14.62 tokens per second)\n",
    "# llama_print_timings:       total time =   26646.69 ms /   372 tokens\n",
    "\n",
    "# gpus and tempreture (12 sec)\n",
    "\n",
    "# llama_print_timings:        load time =     179.98 ms\n",
    "# llama_print_timings:      sample time =     226.22 ms /   386 runs   (    0.59 ms per token,  1706.32 tokens per second)\n",
    "# llama_print_timings: prompt eval time =     179.78 ms /     6 tokens (   29.96 ms per token,    33.37 tokens per second)\n",
    "# llama_print_timings:        eval time =   27294.12 ms /   385 runs   (   70.89 ms per token,    14.11 tokens per second)\n",
    "# llama_print_timings:       total time =   28994.88 ms /   391 tokens\n",
    "\n",
    "# draft model + gpus layers +tempreture (1mint:12sec)\n",
    "\n",
    "# llama_print_timings:        load time =     193.84 ms\n",
    "# llama_print_timings:      sample time =     141.49 ms /   283 runs   (    0.50 ms per token,  2000.17 tokens per second)\n",
    "# llama_print_timings: prompt eval time =   30508.97 ms /  1190 tokens (   25.64 ms per token,    39.00 tokens per second)\n",
    "# llama_print_timings:        eval time =    9525.25 ms /   134 runs   (   71.08 ms per token,    14.07 tokens per second)\n",
    "# llama_print_timings:       total time =   43754.12 ms /  1324 tokens\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
