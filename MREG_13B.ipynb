{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c17a9da",
   "metadata": {},
   "source": [
    "# Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install accelerate==0.18.0\n",
    "!pip install appdirs==1.4.4\n",
    "!pip install bitsandbytes==0.37.2\n",
    "!pip install datasets==2.10.1\n",
    "!pip install fire==0.5.0\n",
    "!pip install git+https://github.com/huggingface/peft.git\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install torch==2.0.0\n",
    "!pip install sentencepiece==0.1.97\n",
    "!pip install tensorboardX==2.6\n",
    "!pip install gradio==3.23.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e704d6",
   "metadata": {},
   "source": [
    "# Login to HuggingFace Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf37f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b88d22",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import textwrap\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "import gc\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "#     prepare_model_for_int8_training,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "# import fire\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "# # import seaborn as sns\n",
    "# from pylab import rcParams\n",
    "\n",
    "# %matplotlib inline\n",
    "# # sns.set(rc={'figure.figsize':(10, 7)})\n",
    "# # sns.set(rc={'figure.dpi':100})\n",
    "# # sns.set(style='white', palette='muted', font_scale=1.2)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8516e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade peft\n",
    "# pip install -i https://test.pypi.org/simple/ bitsandbytes\n",
    "# !pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "# !python -m bitsandbytes\n",
    "# !pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215d7a84",
   "metadata": {},
   "source": [
    "# Load the llama-2–13b-chat-hf model and the corresponding tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b688903",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "# BASE_MODEL = \"meta-llama/Llama-2-7b-chat-hf\" \n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True, # loads the model using 8-bit quantization to reduce memory usage and improve inference speed\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39195f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"mregexperiments-13b-32-syn2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef12c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR=\"CACHE_DIR\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"\n",
    "# tokenizer.save_pretrained(\"mregexperiments-13b-32-syn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc9034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fsspec==2023.9.2 \n",
    "# !pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002fa3dc",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8850a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_files=\"Synth_HumanGenMRE.json\")\n",
    "data[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad488de",
   "metadata": {},
   "source": [
    "# Creating prompts from the loaded dataset and tokenize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22556398",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_LEN=2000\n",
    "\n",
    "# takes a data point from the dataset and generates a prompt by combining the instruction, \n",
    "# input, and output values\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    "\n",
    "# takes the generated prompt and tokenizes it using the tokenizer defined earlier. \n",
    "# It also adds an end-of-sequence token to the input sequence and sets the label to \n",
    "# be the same as the input sequence.\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=True,\n",
    "        return_tensors=None,\n",
    "    )    \n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < CUTOFF_LEN\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "# combines the first two functions to generate and tokenize the prompt in one step\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4113f8",
   "metadata": {},
   "source": [
    "# Splitting the dataset into training, validation and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6563e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=0.2, shuffle=True, seed=42\n",
    ")\n",
    "val_test = train_val[\"test\"].train_test_split(\n",
    "    test_size=0.2, shuffle=True, seed=42\n",
    ")\n",
    "train_data = (\n",
    "    train_val[\"train\"].map(generate_and_tokenize_prompt)\n",
    ")\n",
    "val_data = (\n",
    "    val_test[\"train\"].map(generate_and_tokenize_prompt)\n",
    ")\n",
    "test_data = (\n",
    "    val_test[\"test\"].map(generate_and_tokenize_prompt)\n",
    ")\n",
    "\n",
    "train_data,val_data, test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5abe33e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d5d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training process requires several parameters\n",
    "\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT= 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "#     \"k_proj\",\n",
    "    \"v_proj\",\n",
    "#     \"o_proj\",\n",
    "]\n",
    "BATCH_SIZE = 4 # number of prompt's chunks\n",
    "MICRO_BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LEARNING_RATE = 3e-4 #1e-5 \n",
    "TRAIN_STEPS = 300\n",
    "# EPOCHS=1\n",
    "OUTPUT_DIR = \"MREG-Orig-LORA8-13b-4batches-300epochs-q8-3e-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62943ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with the LORA algorithm, which is a form of quantization that can reduce model size \n",
    "# and memory usage without significant loss in accuracy\n",
    "\n",
    "from accelerate import Accelerator\n",
    "model=prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f41081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifies various settings and hyperparameters for training the model\n",
    "training_arguments = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,#Number of updates steps to accumulate gradients before performing a backward/update pass\n",
    "    warmup_steps=100,#Number of warmup steps for the optimizer\n",
    "    max_steps=TRAIN_STEPS, #The total number of training steps to perform\n",
    "#     num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE, #The learning rate for the optimizer\n",
    "    fp16=True, #Use 16-bit precision for training.\n",
    "    logging_steps=10,\n",
    "    prediction_loss_only=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    save_steps=10,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cac542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates batches of input/output sequences for sequence-to-sequence (seq2seq) models.\n",
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f66d2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For the training process, we'll use the Trainer class from the Hugging Face Transformers library:\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "#     compute_metrics = compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "model.config.use_cache = False\n",
    "old_state_dict = model.state_dict\n",
    " \n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# Clean up the memory using the garbage cleaner\n",
    "gc.collect() \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "trainer.save_model(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca7b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model using validation data: finding loss value\n",
    "import math\n",
    "eval_output = trainer.evaluate()\n",
    "print(\"Validation_output: \",eval_output)\n",
    "perplexity = math.exp(eval_output[\"eval_loss\"])\n",
    "print('\\nEvaluate Perplexity: {:10,.2f}'.format(perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71fbf55",
   "metadata": {},
   "outputs": [],
   "source": [
    " pre= trainer.predict(test_data)\n",
    "print(\"PredictionOutput: \",pre)\n",
    "test_perplexity = math.exp(pre[2]['test_loss'])\n",
    "print('\\nTest Perplexity: {:10,.2f}'.format(test_perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cebb98",
   "metadata": {},
   "source": [
    "# Plotting loss and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d306f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "from ml_things import plot_dict, fix_text# Keep track of train and evaluate loss.\n",
    "loss_history = {'train_loss':[], 'eval_loss':[]}\n",
    " \n",
    "\n",
    "# Keep track of train and evaluate perplexity.\n",
    "# This is a metric useful to track for language models.\n",
    "perplexity_history = {'train_perplexity':[], 'eval_perplexity':[]}\n",
    " \n",
    "# Loop through each log history.\n",
    "for log_history in trainer.state.log_history:\n",
    " \n",
    "    if 'loss' in log_history.keys():\n",
    "    # Deal with trianing loss.\n",
    "        loss_history['train_loss'].append(log_history['loss'])\n",
    "        perplexity_history['train_perplexity'].append(math.exp(log_history['loss']))\n",
    "\n",
    "    elif 'eval_loss' in log_history.keys():\n",
    "    # Deal with eval loss.\n",
    "        loss_history['eval_loss'].append(log_history['eval_loss'])\n",
    "        perplexity_history['eval_perplexity'].append(math.exp(log_history['eval_loss']))\n",
    " \n",
    " # Plot Losses.\n",
    "plot_dict(loss_history, start_step=training_arguments.logging_steps, \n",
    "          step_size=training_arguments.logging_steps, use_title='Loss', \n",
    "          use_xlabel='Train Steps', use_ylabel='Values', magnify=2)\n",
    " \n",
    "print()\n",
    " \n",
    "# Plot Perplexities.\n",
    "plot_dict(perplexity_history, start_step=training_arguments.logging_steps, \n",
    "          step_size=training_arguments.logging_steps, use_title='Perplexity', \n",
    "          use_xlabel='Train Steps', use_ylabel='Values', magnify=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ce078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushing the model to HF\n",
    "model.push_to_hub(\"Nadahass/MREG-Orig-LORA8-13b-4batches-300epochs-q8-3e-4\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa4081f",
   "metadata": {},
   "source": [
    "# Interpretations\n",
    "The learning curve of the model has a moderately high training loss at the beginning which gradually decreases upon adding training examples and flattens gradually, indicating addition of more training examples doesn’t improve the model performance on training data.\n",
    "\n",
    "Also, both the training and validation loss moved close to each other. This is an indicator of having a reasonable number of training data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b250f8",
   "metadata": {},
   "source": [
    "# Inferences and Testing Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc6ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "# from HF\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Nadahass/MREG-13B-GGUF\")\n",
    "# from local\n",
    "model = LlamaForCausalLM.from_pretrained(\"MREG-Orig-LORA8-13b-4batches-300epochs-q8-3e-4\", load_in_8bit=False,torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aafba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"Nadahass/MREG-13B\")\n",
    "tokenizer.pad_token_id = (0  # unk. we want this to be different from the eos token\n",
    "                         )\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers.generation.utils import GreedySearchDecoderOnlyOutput\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE=f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501\n",
    "### Instruction:\n",
    "[instruction]\n",
    "### Input:\n",
    "[input]\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "def create_prompt(instruction: str, inputs:str) -> str:\n",
    "    return PROMPT_TEMPLATE.replace(\"[instruction]\", instruction).replace(\"[input]\",inputs)\n",
    " \n",
    "\n",
    "def generate_response(prompt: str, model: PeftModel) -> GreedySearchDecoderOnlyOutput:\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"].to('cuda')\n",
    " \n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=0.75,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    with torch.inference_mode():\n",
    "        return model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=1000,\n",
    "          \n",
    "        )\n",
    "    \n",
    "def format_response(response: GreedySearchDecoderOnlyOutput) -> str:\n",
    "    decoded_output = tokenizer.decode(response.sequences[0])\n",
    "#     print(\"decoded_output: \", decoded_output)\n",
    "    response = decoded_output.split(\"### Response:\")[1].strip()\n",
    "#     print(\"response: \", response)\n",
    "    return \"\\n\".join(textwrap.wrap(response))\n",
    "\n",
    "\n",
    "def ask_Diana(instr: str, prompt: str, model: PeftModel = model) -> str:\n",
    "    prompt = create_prompt(instr, prompt)\n",
    "    response = generate_response(prompt, model)\n",
    "    return format_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedfd64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score,log_loss\n",
    "\n",
    "refinput=[]\n",
    "\n",
    "references=[]\n",
    "predictions=[]\n",
    "\n",
    "pred_speech=[]\n",
    "ref_speech=[]\n",
    "\n",
    "ref_mod=[]\n",
    "pred_mod=[]\n",
    "\n",
    "ref_pos=[]\n",
    "pred_pos=[]\n",
    "\n",
    "ref_dem=[]\n",
    "pred_dem=[]\n",
    "\n",
    "ref_ids=[]\n",
    "pred_ids=[]\n",
    "\n",
    "\n",
    "\n",
    "count=0\n",
    "\n",
    "\n",
    "while(count < 887):\n",
    "# storing references for val_data\n",
    "    print(\"Count#: \",count+1)\n",
    "    print(\"ref_input: \", test_data[count]['input'])\n",
    "    print(\"ref_output: \", test_data[count]['output'])\n",
    "    print(\"ref part: \", str(test_data[count]['output']).split(','))\n",
    "\n",
    "    refinput.append(test_data[count]['input'])\n",
    "    references.append(test_data[count]['output'])\n",
    "    ref_dem.append(str(test_data[count]['output']).split(',')[1])\n",
    "    ref_mod.append(str(test_data[count]['output']).split(',')[2])\n",
    "    ref_pos.append(str(test_data[count]['output']).split(',')[3])\n",
    "    ref_speech.append(str(test_data[count]['output']).split(',')[0])\n",
    "\n",
    "# storing predictions for val_data prompts   \n",
    "    string=ask_Diana(test_data[count]['instruction'],test_data[count]['input'])\n",
    "    if(\"</s>\" in string):\n",
    "        string=string.replace(\"</s>\",\"\")\n",
    "    if(\"\\n\" in string):\n",
    "        string=string.replace(\"\\n\",\" \")\n",
    "    print(\"predicted_output: \", string)\n",
    "    pred_ids.append(tokenizer(string).input_ids)\n",
    "    predictions.append(string)\n",
    "    if len(string.split(','))==4:\n",
    "        pred_speech.append(string.split(',')[0])\n",
    "        pred_dem.append(string.split(',')[1])\n",
    "        pred_mod.append(string.split(',')[2])\n",
    "        pred_pos.append(string.split(',')[3])\n",
    "    else: \n",
    "        del ref_dem[-1] \n",
    "        del ref_mod[-1]\n",
    "        del ref_pos[-1] \n",
    "        del ref_speech[-1]\n",
    "        \n",
    "    count=count+1\n",
    "    print()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5154a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "length= len(ref_pos)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cc0fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "count =0\n",
    "with open('MREG-Orig-LORA8-13b-4batches-300-q8-3e-4_SynTesting.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['References_input', 'References_output', 'Predictions','Ref_speech','PredSpeech', 'Ref_mod', 'Pred_mod', \n",
    "                  'Ref_dem', 'Pred_dem', 'Ref_pos', 'pred_pos']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    while count < length:\n",
    "        writer.writerow({'References_input':refinput[count],'References_output':references[count], 'Predictions':predictions[count],'Ref_speech':ref_speech[count],\n",
    "                         'PredSpeech':pred_speech[count], 'Ref_mod':ref_mod[count],'Pred_mod':pred_mod[count],\n",
    "                         'Ref_dem':ref_dem[count], 'Pred_dem':pred_dem[count], 'Ref_pos':ref_pos[count], \n",
    "                         'pred_pos':pred_pos[count]})\n",
    "        count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48800e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_word_in_list(sentence, word_list):\n",
    "    # Split the sentence into words\n",
    "    words_in_sentence = sentence.split()\n",
    "    # Check if any word in the sentence exists in the word list\n",
    "    result = any(word in word_list for word in words_in_sentence)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6aa115",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import importlib.metadata as importlib_metadata\n",
    "testsamples=length\n",
    "\n",
    "pointing=0\n",
    "speech=0\n",
    "multi=0\n",
    "\n",
    "rpointing=0\n",
    "rspeech=0\n",
    "rmulti=0\n",
    "\n",
    "pointing_pointing=0\n",
    "speech_speech=0\n",
    "multi_multi=0\n",
    "pointing_speech=0\n",
    "speech_pointing=0\n",
    "multi_pointing=0\n",
    "pointing_multi=0\n",
    "speech_multi=0\n",
    "multi_speech=0\n",
    "\n",
    "ref_multi_rel=0\n",
    "ref_speech_rel=0\n",
    "\n",
    "pred_multi_rel=0\n",
    "pred_speech_rel=0\n",
    "\n",
    "ref_multi_att=0\n",
    "ref_speech_att=0\n",
    "\n",
    "pred_multi_att=0\n",
    "pred_speech_att=0\n",
    "\n",
    "\n",
    "incor_positions=[]\n",
    "\n",
    "ref_spch_spch=[]\n",
    "pred_spch_spch=[]\n",
    "\n",
    "ref_relational=[]\n",
    "pred_relational=[]\n",
    "\n",
    "ref_attributive=[]\n",
    "pred_attributive=[]\n",
    "\n",
    "relat_index1=[]\n",
    "alt_index1=[]\n",
    "\n",
    "relat_index2=[]\n",
    "alt_index2=[]\n",
    "\n",
    "relations = [\"touching\",\n",
    "                \"in\",\n",
    "                \"on\",\n",
    "                \"top\", \n",
    "                \"at\",\n",
    "                \"behind\",\n",
    "                \"in front of\",\n",
    "                \"beside\",\n",
    "                \"near\",\n",
    "                \"left of\",\n",
    "                \"right of\",\n",
    "                \"center of\",\n",
    "                \"edge of\",\n",
    "                \"under\",\n",
    "                \"underneath\",\n",
    "                \"below\",\n",
    "                \"against\",\n",
    "                \"here\",\n",
    "                \"there\",\n",
    "                \"right\",\n",
    "                \"left\",\n",
    "                 \"next\"]\n",
    "\n",
    "# ..................................................  modalities statistics ..................................................\n",
    "for i in pred_mod:\n",
    "    if \"pointing only\" in i:\n",
    "        pointing =pointing +1\n",
    "    if \"speech only\" in i:\n",
    "        speech =speech +1\n",
    "    if \"multimodal\" in i:\n",
    "        multi =multi +1    \n",
    "\n",
    "for j in ref_mod:\n",
    "    if \"pointing only\" in j:\n",
    "        rpointing =rpointing +1\n",
    "    if \"speech only\" in j:\n",
    "        rspeech =rspeech +1\n",
    "    if \"multimodal\" in j:\n",
    "        rmulti =rmulti +1   \n",
    "        \n",
    "for i,j in zip(ref_mod,pred_mod):\n",
    "    if \"pointing only\" in i and \"pointing only\" in j:\n",
    "        pointing_pointing =pointing_pointing +1\n",
    "    if \"speech only\" in i and \"speech only\" in j:\n",
    "        speech_speech =speech_speech +1\n",
    "    if \"multimodal\" in i and \"multimodal\" in j:\n",
    "        multi_multi =multi_multi +1  \n",
    "    if \"pointing only\" in i and \"speech only\" in j:\n",
    "        pointing_speech =pointing_speech +1  \n",
    "    if \"speech only\" in i and \"pointing only\" in j:\n",
    "        speech_pointing =speech_pointing +1 \n",
    "    if \"multimodal\" in i and \"pointing only\" in j:\n",
    "        multi_pointing =multi_pointing +1  \n",
    "    if \"pointing only\" in i and \"multimodal\" in j:\n",
    "        pointing_multi =pointing_multi +1  \n",
    "    if \"speech only\" in i and \"multimodal\" in j:\n",
    "        speech_multi =speech_multi +1  \n",
    "    if \"multimodal\" in i and \"speech only\" in j:\n",
    "        multi_speech =multi_speech+1      \n",
    "        \n",
    " # ..................................................  speech statistics ..................................................\n",
    "\n",
    "\n",
    "for i,j in zip(ref_speech,pred_speech):\n",
    "    if \"No speech\" not in i and \"No speech\" not in j:\n",
    "        ref_spch_spch.append(i)\n",
    "        pred_spch_spch.append(j)\n",
    "\n",
    " # ..................................................  relational and attributive speech statistics ..................................................\n",
    "\n",
    "for re1 in ref_speech:\n",
    "    if search_word_in_list(re1, relations):\n",
    "        ref_relational.append(re1)\n",
    "        relat_index2.append(ref_speech.index(re1))\n",
    "    elif search_word_in_list(re1, relations)==False and \"No speech\" not in re1:\n",
    "        ref_attributive.append(re1)\n",
    "        alt_index2.append(ref_speech.index(re1))\n",
    "\n",
    "        \n",
    "for re2 in pred_speech:\n",
    "    if search_word_in_list(re2, relations):\n",
    "        pred_relational.append(re2)\n",
    "        relat_index1.append(pred_speech.index(re2))\n",
    "    elif search_word_in_list(re2, relations)==False and \"No speech\" not in re2:\n",
    "        pred_attributive.append(re2)\n",
    "        alt_index1.append(pred_speech.index(re2))\n",
    "        \n",
    "\n",
    "# ref multimodal -relational\n",
    "# ref speech only -relational\n",
    "for s1 in relat_index2:\n",
    "    if(\"multimodal\" in ref_mod[s1]):\n",
    "        ref_multi_rel=ref_multi_rel+1\n",
    "    \n",
    "\n",
    "for s2 in relat_index2:\n",
    "    if(\"speech only\" in ref_mod[s2]):\n",
    "        ref_speech_rel=ref_speech_rel+1\n",
    "    \n",
    "\n",
    "# pred multimodal -relational\n",
    "# pred speech only -relational  \n",
    "\n",
    "for p1 in relat_index1:\n",
    "    if(\"multimodal\" in pred_mod[p1]):\n",
    "        pred_multi_rel=pred_multi_rel+1\n",
    "    \n",
    "for p2 in relat_index1:\n",
    "    if(\"speech only\" in pred_mod[p2]):\n",
    "        pred_speech_rel=pred_speech_rel+1\n",
    "    \n",
    "        \n",
    "# ref multimodal - attributive\n",
    "# ref speech only - attributive\n",
    "\n",
    "for a1 in alt_index2:\n",
    "    if(\"multimodal\" in ref_mod[a1]):\n",
    "        ref_multi_att=ref_multi_att+1\n",
    "    \n",
    "\n",
    "for a2 in alt_index2:\n",
    "    if(\"speech only\" in ref_mod[a2]):\n",
    "        ref_speech_att=ref_speech_att+1\n",
    "    \n",
    "# pred multimodal -attributive\n",
    "# pred speech only -attributive      \n",
    "    \n",
    "for b1 in alt_index1:\n",
    "    if(\"multimodal\" in pred_mod[b1]):\n",
    "        pred_multi_att=pred_multi_att+1\n",
    "    \n",
    "\n",
    "for b2 in alt_index1:\n",
    "    if(\"speech only\" in pred_mod[b2]):\n",
    "        pred_speech_att=pred_speech_att+1\n",
    "\n",
    "        \n",
    "#   ......................................Position investigation........................................ \n",
    "\n",
    "for p1,p2 in zip(ref_pos,pred_pos):\n",
    "    if (p1 != p2): \n",
    "        incor_positions.append(\" REF: \"+ ref_speech[ref_pos.index(p1)] + \" , \" + ref_mod[ref_pos.index(p1)] + \" , \" + ref_pos[ref_pos.index(p1)] +\" PRED: \"+ pred_speech[ref_pos.index(p1)] + \" , \" + pred_mod[ref_pos.index(p1)] + \" , \" + pred_pos[ref_pos.index(p1)])                       \n",
    "    \n",
    "# sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "# sacrebleu_results=sacrebleu.compute(predictions=pred_speech, references=ref_speech)\n",
    "\n",
    "# rouge = evaluate.load('rouge')\n",
    "# rouge_results=rouge.compute(predictions=pred_speech, references=ref_speech)\n",
    "\n",
    "# METEOR = evaluate.load('meteor')\n",
    "# METEOR_results=METEOR.compute(predictions=pred_speech, references=ref_speech)\n",
    "\n",
    "\n",
    "bertscore_metric = load_metric('bertscore')\n",
    "bert_scores = bertscore_metric.compute(predictions=pred_speech, references=ref_speech, lang=\"en\")\n",
    "\n",
    "\n",
    "bert_scores2 = bertscore_metric.compute(predictions=pred_spch_spch, references=ref_spch_spch, lang=\"en\")\n",
    "\n",
    "# bleurt_metric = load_metric('bleurt')\n",
    "# bleurt_scores = bleurt_metric.compute(predictions=pred_speech, references=ref_speech)\n",
    "\n",
    "# accuracy_results=accuracy_score(predictions, references)\n",
    "# precision_results=precision_score(references, predictions,average = \"micro\",zero_division=1)\n",
    "# recall_results=recall_score(references, predictions,average =\"micro\",zero_division=1)\n",
    "# f1 =f1_score(references, predictions, average = \"micro\")\n",
    "\n",
    "sp_accuracy_results=accuracy_score(ref_speech, pred_speech)\n",
    "sp_precision_results=precision_score(ref_speech, pred_speech,average = \"micro\",zero_division=1)\n",
    "sp_recall_results=recall_score(ref_speech, pred_speech,average =\"micro\",zero_division=1)\n",
    "sp_f1 =f1_score(ref_speech, pred_speech, average = \"micro\")\n",
    "\n",
    "dem_accuracy_results=accuracy_score(ref_dem, pred_dem)\n",
    "dem_precision_results=precision_score(ref_dem, pred_dem,average = \"micro\",zero_division=1)\n",
    "dem_recall_results=recall_score(ref_dem, pred_dem,average =\"micro\",zero_division=1)\n",
    "dem_f1 =f1_score(ref_dem, pred_dem, average = \"micro\")\n",
    "\n",
    "mod_accuracy_results=accuracy_score(ref_mod, pred_mod)\n",
    "mod_precision_results=precision_score(ref_mod, pred_mod,average = \"micro\",zero_division=1)\n",
    "mod_recall_results=recall_score(ref_mod, pred_mod,average =\"micro\",zero_division=1)\n",
    "mod_f1 =f1_score(ref_mod, pred_mod, average = \"micro\")\n",
    "\n",
    "pos_accuracy_results=accuracy_score(ref_pos, pred_pos)\n",
    "pos_precision_results=precision_score(ref_pos, pred_pos,average = \"micro\",zero_division=0)\n",
    "pos_recall_results=recall_score(ref_pos, pred_pos,average =\"micro\",zero_division=1)\n",
    "pos_f1 =f1_score(ref_pos, pred_pos, average = \"micro\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Number of reference pointing only REs: \", rpointing)\n",
    "print(\"Number of reference speech only REs: \", rspeech)\n",
    "print(\"Number of reference multimodal only REs: \", rmulti)\n",
    "print()\n",
    "print(\"Number of predicted pointing only REs: \", pointing)\n",
    "print(\"Number of predicted speech only REs: \", speech)\n",
    "print(\"Number of predicted multimodal only REs: \", multi)\n",
    "print()\n",
    "print(\"Number of identical pointing only REs for ref aand pred: \", pointing_pointing)\n",
    "print(\"Number of identical speech only REs for ref aand pred: \", speech_speech)\n",
    "print(\"Number of identical multimodal  REs for ref aand pred: \", multi_multi)\n",
    "print()\n",
    "print(\"Number of pointing speech REs for ref aand pred: \", pointing_speech)\n",
    "print(\"Number of speech_pointing REs for ref aand pred: \", speech_pointing)\n",
    "print()\n",
    "print(\"Number of multi_pointing REs for ref aand pred: \", multi_pointing)\n",
    "print(\"Number of pointing_multi REs for ref aand pred: \", pointing_multi)\n",
    "print()\n",
    "print(\"Number of speech_multi REs for ref aand pred: \", speech_multi)\n",
    "print(\"Number of multi_speech REs for ref aand pred: \", multi_speech)\n",
    "print()\n",
    "print(\"ref_multi_rel: \", ref_multi_rel)\n",
    "print(\"ref_speech_rel: \", ref_speech_rel)\n",
    "\n",
    "print(\"pred_multi_rel: \", pred_multi_rel)\n",
    "print(\"pred_speech_rel: \", pred_speech_rel)\n",
    "print()\n",
    "\n",
    "print(\"ref_multi_att: \", ref_multi_att)\n",
    "print(\"ref_speech_att: \", ref_speech_att)\n",
    "print()\n",
    "\n",
    "print(\"pred_multi_att: \", pred_multi_att)\n",
    "print(\"pred_speech_att: \", pred_speech_att)\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Evaluation Using Testing Data\")\n",
    "print()\n",
    "\n",
    "#N-gram based metrics have a critical disadvantage which only uses the sole characteristic of word matching. \n",
    "#In which, this cannot evaluate the true semantics of the whole sentence to have a fair comparison.\n",
    "print(\"N-gram based metrics\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print()\n",
    "\n",
    "# print(\"Bleu: \", sacrebleu_results[\"score\"])\n",
    "# print(\"Rouge: \", rouge_results)\n",
    "# print(\"METEOR: \", METEOR_results)\n",
    "print()\n",
    "# print(\"Tuple Accuracy: \", accuracy_results)\n",
    "# print(\"Tuple Precision: \", precision_results)\n",
    "# print(\"Tuple Recall: \", recall_results)\n",
    "# print(\"Tuple F1: \", f1)\n",
    "print()\n",
    "print(\"Speech Accuracy: \", sp_accuracy_results)\n",
    "print(\"Speech Precision: \", sp_precision_results)\n",
    "print(\"Speech Recall: \", sp_recall_results)\n",
    "print(\"Speech F1: \", sp_f1)\n",
    "print()\n",
    "print(\"Dem Accuracy: \", dem_accuracy_results)\n",
    "print(\"Dem Precision: \", dem_precision_results)\n",
    "print(\"Dem Recall: \", dem_recall_results)\n",
    "print(\"Dem F1: \", dem_f1)\n",
    "print()\n",
    "print(\"Mod Accuracy: \", mod_accuracy_results)\n",
    "print(\"Mod Precision: \", mod_precision_results)\n",
    "print(\"Mod Recall: \", mod_recall_results)\n",
    "print(\"Mod F1: \", mod_f1)\n",
    "print()\n",
    "print(\"Pos Accuracy: \", pos_accuracy_results)\n",
    "print(\"Pos Precision: \", pos_precision_results)\n",
    "print(\"Pos Recall: \", pos_recall_results)\n",
    "print(\"Pos F1: \", pos_f1)\n",
    "\n",
    "print(\"Incorrect postion predition details: \")\n",
    "for inc in incor_positions:\n",
    "    print(inc)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"MODEL BASED METRICS\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print()\n",
    "#To retrieve the true semantics of a sentence, BERTScore leverages Transformer based model BERT embeddings. \n",
    "#After feeding the candidate and reference sentences through the model, \n",
    "#it calculates the similarity of the two sentences with cosine-similarity and uses importance weighting \n",
    "#to get the final scores.\n",
    "#PRECISION, RECALL, F1 for each data point\n",
    "#print(\"Bert_score (): \", bert_scores['precision'][0])\n",
    "\n",
    "print(\"Bert_score : \", bert_scores) \n",
    "\n",
    "total1=0\n",
    "for pre in bert_scores['precision']:\n",
    "    total1= total1 + pre\n",
    "average1=total1/testsamples\n",
    "print()\n",
    "print(\"All_Bert_precision average: \", average1)\n",
    "\n",
    "total1=0\n",
    "average1=0\n",
    "for re in bert_scores['recall']:\n",
    "    total1= total1 + re\n",
    "average1=total1/testsamples\n",
    "print()\n",
    "print(\"All_Bert_recall average: \", average1)\n",
    "\n",
    "total1=0\n",
    "average1=0\n",
    "for f1 in bert_scores['f1']:\n",
    "    total1= total1 + f1\n",
    "average1=total1/testsamples\n",
    "print()\n",
    "print(\"All_Bert_f1 average: \", average1)\n",
    "\n",
    "# ..............................................speech only.......................................................\n",
    "\n",
    "print(\"Bert_score : \", bert_scores2) \n",
    "total1=0\n",
    "for pre in bert_scores2['precision']:\n",
    "    total1= total1 + pre\n",
    "average1=total1/len(pred_spch_spch)\n",
    "print()\n",
    "print(\"speech only_Bert_precision average: \", average1)\n",
    "\n",
    "total1=0\n",
    "average1=0\n",
    "for re in bert_scores2['recall']:\n",
    "    total1= total1 + re\n",
    "average1=total1/len(pred_spch_spch)\n",
    "print()\n",
    "print(\"speech only_Bert_recall average: \", average1)\n",
    "\n",
    "total1=0\n",
    "average1=0\n",
    "for f1 in bert_scores2['f1']:\n",
    "    total1= total1 + f1\n",
    "average1=total1/len(pred_spch_spch)\n",
    "print()\n",
    "print(\"speech only_Bert_f1 average: \", average1)\n",
    "\n",
    "#BLEURT is a BERT-based model which is pre-trained on synthetic data and fine-tuned on a small rating dataset \n",
    "#to acquire human judgement knowledge\n",
    "print()\n",
    "# total2=0\n",
    "# print(\"Bleurt_score: \", bleurt_scores)\n",
    "# for bleu in bleurt_scores['scores']:\n",
    "#     total2= total2 + bleu    \n",
    "# average2=total2/testsamples\n",
    "# print()\n",
    "# print(\"Bleurt_score average: \", average2)\n",
    "\n",
    "# from datasets import list_metrics\n",
    "# metrics_list = list_metrics()\n",
    "# print(\"metrics_list:\" ,metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c7d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersection over Union (IoU) score\n",
    "def calculate_iou(sentence1, sentence2):\n",
    "    # Convert sentences to sets of words\n",
    "    set1 = set(sentence1.split())\n",
    "    set2 = set(sentence2.split())\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou = intersection / union if union != 0 else 0\n",
    "    \n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ebcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "total3=0\n",
    "with open('IoU-Orig-LORA8-13b-4batches-300epochs-q8-3e-4_synTesting.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['References', 'Predictions','IoU']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for p,r in zip(predictions, references):\n",
    "        iou_score = calculate_iou(p, r)\n",
    "        writer.writerow({'References':r, 'Predictions':p,'IoU':iou_score})\n",
    "        total3=total3+iou_score\n",
    "\n",
    "iou_avg=total3/length\n",
    "print(\"Average of (IoU) score:\", iou_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c297195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model in gguf format either from HF or local\n",
    "from llama_cpp import Llama\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "# Huggingface\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Nadahass/MREG-13B-GGUF\", model_file=\"mreg-13b-32-hf.gguf\", model_type=\"llama\", gpu_layers=50)\n",
    "#local\n",
    "model = Llama(model_path=\"mreg-13b-32-hf.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a3b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =create_prompt(\"Generate a referring expression for an object.\",\"PinkBlock2 , (0.295275000: 1.224503000: 0.150793400) , None , ( 0.295275: 1.149781: 0.1507934) ) , None , None ,  put put grasp put put put put put put grasp put put put grasp put put put put , PinkBlock2 BlueBlock1 BlueBlock1 RedBlock2 BlueBlock2 YellowBlock1 YellowBlock2 YellowBlock1 RedBlock2 RedBlock2 RedBlock2 BlueBlock2 GreenBlock2 GreenBlock2 PinkBlock2 BlueBlock2 PinkBlock2 BlueBlock1\")\n",
    "print(model(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
