{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c17a9da",
   "metadata": {},
   "source": [
    "# Install required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de2290",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install accelerate==0.18.0\n",
    "!pip install appdirs==1.4.4\n",
    "!pip install bitsandbytes==0.37.2\n",
    "!pip install datasets==2.10.1\n",
    "!pip install fire==0.5.0\n",
    "!pip install git+https://github.com/huggingface/peft.git\n",
    "!pip install git+https://github.com/huggingface/transformers.git\n",
    "!pip install torch==2.0.0\n",
    "!pip install sentencepiece==0.1.97\n",
    "!pip install tensorboardX==2.6\n",
    "!pip install gradio==3.23.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e704d6",
   "metadata": {},
   "source": [
    "# Login to HuggingFace Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf37f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b88d22",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26de638c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import textwrap\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "import gc\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "#     prepare_model_for_int8_training,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "# import fire\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import matplotlib as mpl\n",
    "# # import seaborn as sns\n",
    "# from pylab import rcParams\n",
    "\n",
    "# %matplotlib inline\n",
    "# # sns.set(rc={'figure.figsize':(10, 7)})\n",
    "# # sns.set(rc={'figure.dpi':100})\n",
    "# # sns.set(style='white', palette='muted', font_scale=1.2)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8516e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade peft\n",
    "# pip install -i https://test.pypi.org/simple/ bitsandbytes\n",
    "# !pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "# !python -m bitsandbytes\n",
    "# !pip install -i https://pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215d7a84",
   "metadata": {},
   "source": [
    "# Load the llama-2â€“13b-chat-hf model and the corresponding tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b688903",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "# BASE_MODEL = \"meta-llama/Llama-2-7b-chat-hf\" \n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True, # loads the model using 8-bit quantization to reduce memory usage and improve inference speed\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39195f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"mregexperiments-13b-32-syn2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef12c1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR=\"CACHE_DIR\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\"\n",
    "# tokenizer.save_pretrained(\"mregexperiments-13b-32-syn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc9034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install fsspec==2023.9.2 \n",
    "# !pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002fa3dc",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8850a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"json\", data_files=\"Synth_HumanGenMRE.json\")\n",
    "data[\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad488de",
   "metadata": {},
   "source": [
    "# Creating prompts from the loaded dataset and tokenize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22556398",
   "metadata": {},
   "outputs": [],
   "source": [
    "CUTOFF_LEN=2000\n",
    "\n",
    "# takes a data point from the dataset and generates a prompt by combining the instruction, \n",
    "# input, and output values\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501\n",
    "### Instruction:\n",
    "{data_point[\"instruction\"]}\n",
    "### Input:\n",
    "{data_point[\"input\"]}\n",
    "### Response:\n",
    "{data_point[\"output\"]}\"\"\"\n",
    "\n",
    "# takes the generated prompt and tokenizes it using the tokenizer defined earlier. \n",
    "# It also adds an end-of-sequence token to the input sequence and sets the label to \n",
    "# be the same as the input sequence.\n",
    "\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=CUTOFF_LEN,\n",
    "        padding=True,\n",
    "        return_tensors=None,\n",
    "    )    \n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < CUTOFF_LEN\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "# combines the first two functions to generate and tokenize the prompt in one step\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt = generate_prompt(data_point)\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    return tokenized_full_prompt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4113f8",
   "metadata": {},
   "source": [
    "# Splitting the dataset into training, validation and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6563e06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val = data[\"train\"].train_test_split(\n",
    "    test_size=0.2, shuffle=True, seed=42\n",
    ")\n",
    "val_test = train_val[\"test\"].train_test_split(\n",
    "    test_size=0.2, shuffle=True, seed=42\n",
    ")\n",
    "train_data = (\n",
    "    train_val[\"train\"].map(generate_and_tokenize_prompt)\n",
    ")\n",
    "val_data = (\n",
    "    val_test[\"train\"].map(generate_and_tokenize_prompt)\n",
    ")\n",
    "test_data = (\n",
    "    val_test[\"test\"].map(generate_and_tokenize_prompt)\n",
    ")\n",
    "\n",
    "train_data,val_data, test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5abe33e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d5d38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training process requires several parameters\n",
    "\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT= 0.05\n",
    "LORA_TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "#     \"k_proj\",\n",
    "    \"v_proj\",\n",
    "#     \"o_proj\",\n",
    "]\n",
    "BATCH_SIZE = 4 # number of prompt's chunks\n",
    "MICRO_BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE\n",
    "LEARNING_RATE = 3e-4 #1e-5 \n",
    "TRAIN_STEPS = 300\n",
    "# EPOCHS=1\n",
    "OUTPUT_DIR = \"MREG-Orig-LORA8-13b-4batches-300epochs-q8-3e-4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62943ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with the LORA algorithm, which is a form of quantization that can reduce model size \n",
    "# and memory usage without significant loss in accuracy\n",
    "\n",
    "from accelerate import Accelerator\n",
    "model=prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=LORA_TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f41081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifies various settings and hyperparameters for training the model\n",
    "training_arguments = transformers.TrainingArguments(\n",
    "    per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,#Number of updates steps to accumulate gradients before performing a backward/update pass\n",
    "    warmup_steps=100,#Number of warmup steps for the optimizer\n",
    "    max_steps=TRAIN_STEPS, #The total number of training steps to perform\n",
    "#     num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE, #The learning rate for the optimizer\n",
    "    fp16=True, #Use 16-bit precision for training.\n",
    "    logging_steps=10,\n",
    "    prediction_loss_only=True,\n",
    "    optim=\"adamw_torch\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    save_steps=10,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cac542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates batches of input/output sequences for sequence-to-sequence (seq2seq) models.\n",
    "data_collator = transformers.DataCollatorForSeq2Seq(\n",
    "    tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f66d2e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For the training process, we'll use the Trainer class from the Hugging Face Transformers library:\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "#     compute_metrics = compute_metrics,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "model.config.use_cache = False\n",
    "old_state_dict = model.state_dict\n",
    " \n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# Clean up the memory using the garbage cleaner\n",
    "gc.collect() \n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "trainer.save_model(OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca7b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model using validation data: finding loss value\n",
    "import math\n",
    "eval_output = trainer.evaluate()\n",
    "print(\"Validation_output: \",eval_output)\n",
    "perplexity = math.exp(eval_output[\"eval_loss\"])\n",
    "print('\\nEvaluate Perplexity: {:10,.2f}'.format(perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71fbf55",
   "metadata": {},
   "outputs": [],
   "source": [
    " pre= trainer.predict(test_data)\n",
    "print(\"PredictionOutput: \",pre)\n",
    "test_perplexity = math.exp(pre[2]['test_loss'])\n",
    "print('\\nTest Perplexity: {:10,.2f}'.format(test_perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cebb98",
   "metadata": {},
   "source": [
    "# Plotting loss and perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d306f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "from ml_things import plot_dict, fix_text# Keep track of train and evaluate loss.\n",
    "loss_history = {'train_loss':[], 'eval_loss':[]}\n",
    " \n",
    "\n",
    "# Keep track of train and evaluate perplexity.\n",
    "# This is a metric useful to track for language models.\n",
    "perplexity_history = {'train_perplexity':[], 'eval_perplexity':[]}\n",
    " \n",
    "# Loop through each log history.\n",
    "for log_history in trainer.state.log_history:\n",
    " \n",
    "    if 'loss' in log_history.keys():\n",
    "    # Deal with trianing loss.\n",
    "        loss_history['train_loss'].append(log_history['loss'])\n",
    "        perplexity_history['train_perplexity'].append(math.exp(log_history['loss']))\n",
    "\n",
    "    elif 'eval_loss' in log_history.keys():\n",
    "    # Deal with eval loss.\n",
    "        loss_history['eval_loss'].append(log_history['eval_loss'])\n",
    "        perplexity_history['eval_perplexity'].append(math.exp(log_history['eval_loss']))\n",
    " \n",
    " # Plot Losses.\n",
    "plot_dict(loss_history, start_step=training_arguments.logging_steps, \n",
    "          step_size=training_arguments.logging_steps, use_title='Loss', \n",
    "          use_xlabel='Train Steps', use_ylabel='Values', magnify=2)\n",
    " \n",
    "print()\n",
    " \n",
    "# Plot Perplexities.\n",
    "plot_dict(perplexity_history, start_step=training_arguments.logging_steps, \n",
    "          step_size=training_arguments.logging_steps, use_title='Perplexity', \n",
    "          use_xlabel='Train Steps', use_ylabel='Values', magnify=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ce078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pushing the model to HF\n",
    "model.push_to_hub(\"Nadahass/MREG-Orig-LORA8-13b-4batches-300epochs-q8-3e-4\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa4081f",
   "metadata": {},
   "source": [
    "# Interpretations\n",
    "The learning curve of the model has a moderately high training loss at the beginning which gradually decreases upon adding training examples and flattens gradually, indicating addition of more training examples doesnâ€™t improve the model performance on training data.\n",
    "\n",
    "Also, both the training and validation loss moved close to each other. This is an indicator of having a reasonable number of training data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b250f8",
   "metadata": {},
   "source": [
    "# Inferences and Testing Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc6ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "# from HF\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Nadahass/MREG-13B-GGUF\")\n",
    "# from local\n",
    "model = LlamaForCausalLM.from_pretrained(\"MREG-Orig-LORA8-13b-4batches-300epochs-q8-3e-4\", load_in_8bit=False,torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aafba9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"Nadahass/MREG-13B\")\n",
    "tokenizer.pad_token_id = (0  # unk. we want this to be different from the eos token\n",
    "                         )\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae4818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers.generation.utils import GreedySearchDecoderOnlyOutput\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE=f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.  # noqa: E501\n",
    "### Instruction:\n",
    "[instruction]\n",
    "### Input:\n",
    "[input]\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "def create_prompt(instruction: str, inputs:str) -> str:\n",
    "    return PROMPT_TEMPLATE.replace(\"[instruction]\", instruction).replace(\"[input]\",inputs)\n",
    " \n",
    "\n",
    "def generate_response(prompt: str, model: PeftModel) -> GreedySearchDecoderOnlyOutput:\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = encoding[\"input_ids\"].to('cuda')\n",
    " \n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=0.75,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "    with torch.inference_mode():\n",
    "        return model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=1000,\n",
    "          \n",
    "        )\n",
    "    \n",
    "def format_response(response: GreedySearchDecoderOnlyOutput) -> str:\n",
    "    decoded_output = tokenizer.decode(response.sequences[0])\n",
    "#     print(\"decoded_output: \", decoded_output)\n",
    "    response = decoded_output.split(\"### Response:\")[1].strip()\n",
    "#     print(\"response: \", response)\n",
    "    return \"\\n\".join(textwrap.wrap(response))\n",
    "\n",
    "\n",
    "def ask_Diana(instr: str, prompt: str, model: PeftModel = model) -> str:\n",
    "    prompt = create_prompt(instr, prompt)\n",
    "    response = generate_response(prompt, model)\n",
    "    return format_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedfd64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score,log_loss\n",
    "\n",
    "refinput=[]\n",
    "\n",
    "references=[]\n",
    "predictions=[]\n",
    "\n",
    "pred_speech=[]\n",
    "ref_speech=[]\n",
    "\n",
    "ref_mod=[]\n",
    "pred_mod=[]\n",
    "\n",
    "ref_pos=[]\n",
    "pred_pos=[]\n",
    "\n",
    "ref_dem=[]\n",
    "pred_dem=[]\n",
    "\n",
    "ref_ids=[]\n",
    "pred_ids=[]\n",
    "\n",
    "\n",
    "\n",
    "count=0\n",
    "\n",
    "\n",
    "while(count < 887):\n",
    "# storing references for val_data\n",
    "    print(\"Count#: \",count+1)\n",
    "    print(\"ref_input: \", test_data[count]['input'])\n",
    "    print(\"ref_output: \", test_data[count]['output'])\n",
    "    print(\"ref part: \", str(test_data[count]['output']).split(','))\n",
    "\n",
    "    refinput.append(test_data[count]['input'])\n",
    "    references.append(test_data[count]['output'])\n",
    "    ref_dem.append(str(test_data[count]['output']).split(',')[1])\n",
    "    ref_mod.append(str(test_data[count]['output']).split(',')[2])\n",
    "    ref_pos.append(str(test_data[count]['output']).split(',')[3])\n",
    "    ref_speech.append(str(test_data[count]['output']).split(',')[0])\n",
    "\n",
    "# storing predictions for val_data prompts   \n",
    "    string=ask_Diana(test_data[count]['instruction'],test_data[count]['input'])\n",
    "    if(\"</s>\" in string):\n",
    "        string=string.replace(\"</s>\",\"\")\n",
    "    if(\"\\n\" in string):\n",
    "        string=string.replace(\"\\n\",\" \")\n",
    "    print(\"predicted_output: \", string)\n",
    "    pred_ids.append(tokenizer(string).input_ids)\n",
    "    predictions.append(string)\n",
    "    if len(string.split(','))==4:\n",
    "        pred_speech.append(string.split(',')[0])\n",
    "        pred_dem.append(string.split(',')[1])\n",
    "        pred_mod.append(string.split(',')[2])\n",
    "        pred_pos.append(string.split(',')[3])\n",
    "    else: \n",
    "        del ref_dem[-1] \n",
    "        del ref_mod[-1]\n",
    "        del ref_pos[-1] \n",
    "        del ref_speech[-1]\n",
    "        \n",
    "    count=count+1\n",
    "    print()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5154a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "length= len(ref_pos)\n",
    "print(length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4cc0fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "count =0\n",
    "with open('MREG-Orig-LORA8-13b-4batches-300-q8-3e-4_SynTesting.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['References_input', 'References_output', 'Predictions','Ref_speech','PredSpeech', 'Ref_mod', 'Pred_mod', \n",
    "                  'Ref_dem', 'Pred_dem', 'Ref_pos', 'pred_pos']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    while count < length:\n",
    "        writer.writerow({'References_input':refinput[count],'References_output':references[count], 'Predictions':predictions[count],'Ref_speech':ref_speech[count],\n",
    "                         'PredSpeech':pred_speech[count], 'Ref_mod':ref_mod[count],'Pred_mod':pred_mod[count],\n",
    "                         'Ref_dem':ref_dem[count], 'Pred_dem':pred_dem[count], 'Ref_pos':ref_pos[count], \n",
    "                         'pred_pos':pred_pos[count]})\n",
    "        count=count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48800e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_word_in_list(sentence, word_list):\n",
    "    # Split the sentence into words\n",
    "    words_in_sentence = sentence.split()\n",
    "    # Check if any word in the sentence exists in the word list\n",
    "    result = any(word in word_list for word in words_in_sentence)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6aa115",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import importlib.metadata as importlib_metadata\n",
    "testsamples=length\n",
    "\n",
    "pointing=0\n",
    "speech=0\n",
    "multi=0\n",
    "\n",
    "rpointing=0\n",
    "rspeech=0\n",
    "rmulti=0\n",
    "\n",
    "pointing_pointing=0\n",
    "speech_speech=0\n",
    "multi_multi=0\n",
    "pointing_speech=0\n",
    "speech_pointing=0\n",
    "multi_pointing=0\n",
    "pointing_multi=0\n",
    "speech_multi=0\n",
    "multi_speech=0\n",
    "\n",
    "ref_multi_rel=0\n",
    "ref_speech_rel=0\n",
    "\n",
    "pred_multi_rel=0\n",
    "pred_speech_rel=0\n",
    "\n",
    "ref_multi_att=0\n",
    "ref_speech_att=0\n",
    "\n",
    "pred_multi_att=0\n",
    "pred_speech_att=0\n",
    "\n",
    "\n",
    "incor_positions=[]\n",
    "\n",
    "ref_spch_spch=[]\n",
    "pred_spch_spch=[]\n",
    "\n",
    "ref_relational=[]\n",
    "pred_relational=[]\n",
    "\n",
    "ref_attributive=[]\n",
    "pred_attributive=[]\n",
    "\n",
    "relat_index1=[]\n",
    "alt_index1=[]\n",
    "\n",
    "relat_index2=[]\n",
    "alt_index2=[]\n",
    "\n",
    "relations = [\"touching\",\n",
    "                \"in\",\n",
    "                \"on\",\n",
    "                \"top\", \n",
    "                \"at\",\n",
    "                \"behind\",\n",
    "                \"in front of\",\n",
    "                \"beside\",\n",
    "                \"near\",\n",
    "                \"left of\",\n",
    "                \"right of\",\n",
    "                \"center of\",\n",
    "                \"edge of\",\n",
    "                \"under\",\n",
    "                \"underneath\",\n",
    "                \"below\",\n",
    "                \"against\",\n",
    "                \"here\",\n",
    "                \"there\",\n",
    "                \"right\",\n",
    "                \"left\",\n",
    "                 \"next\"]\n",
    "\n",
    "# ..................................................  modalities statistics ..................................................\n",
    "for i in pred_mod:\n",
    "    if \"pointing only\" in i:\n",
    "        pointing =pointing +1\n",
    "    if \"speech only\" in i:\n",
    "        speech =speech +1\n",
    "    if \"multimodal\" in i:\n",
    "        multi =multi +1    \n",
    "\n",
    "for j in ref_mod:\n",
    "    if \"pointing only\" in j:\n",
    "        rpointing =rpointing +1\n",
    "    if \"speech only\" in j:\n",
    "        rspeech =rspeech +1\n",
    "    if \"multimodal\" in j:\n",
    "        rmulti =rmulti +1   \n",
    "        \n",
    "for i,j in zip(ref_mod,pred_mod):\n",
    "    if \"pointing only\" in i and \"pointing only\" in j:\n",
    "        pointing_pointing =pointing_pointing +1\n",
    "    if \"speech only\" in i and \"speech only\" in j:\n",
    "        speech_speech =speech_speech +1\n",
    "    if \"multimodal\" in i and \"multimodal\" in j:\n",
    "        multi_multi =multi_multi +1  \n",
    "    if \"pointing only\" in i and \"speech only\" in j:\n",
    "        pointing_speech =pointing_speech +1  \n",
    "    if \"speech only\" in i and \"pointing only\" in j:\n",
    "        speech_pointing =speech_pointing +1 \n",
    "    if \"multimodal\" in i and \"pointing only\" in j:\n",
    "        multi_pointing =multi_pointing +1  \n",
    "    if \"pointing only\" in i and \"multimodal\" in j:\n",
    "        pointing_multi =pointing_multi +1  \n",
    "    if \"speech only\" in i and \"multimodal\" in j:\n",
    "        speech_multi =speech_multi +1  \n",
    "    if \"multimodal\" in i and \"speech only\" in j:\n",
    "        multi_speech =multi_speech+1      \n",
    "        \n",
    " # ..................................................  speech statistics ..................................................\n",
    "\n",
    "\n",
    "for i,j in zip(ref_speech,pred_speech):\n",
    "    if \"No speech\" not in i and \"No speech\" not in j:\n",
    "        ref_spch_spch.append(i)\n",
    "        pred_spch_spch.append(j)\n",
    "\n",
    " # ..................................................  relational and attributive speech statistics ..................................................\n",
    "\n",
    "for re1 in ref_speech:\n",
    "    if search_word_in_list(re1, relations):\n",
    "        ref_relational.append(re1)\n",
    "        relat_index2.append(ref_speech.index(re1))\n",
    "    elif search_word_in_list(re1, relations)==False and \"No speech\" not in re1:\n",
    "        ref_attributive.append(re1)\n",
    "        alt_index2.append(ref_speech.index(re1))\n",
    "\n",
    "        \n",
    "for re2 in pred_speech:\n",
    "    if search_word_in_list(re2, relations):\n",
    "        pred_relational.append(re2)\n",
    "        relat_index1.append(pred_speech.index(re2))\n",
    "    elif search_word_in_list(re2, relations)==False and \"No speech\" not in re2:\n",
    "        pred_attributive.append(re2)\n",
    "        alt_index1.append(pred_speech.index(re2))\n",
    "        \n",
    "\n",
    "# ref multimodal -relational\n",
    "# ref speech only -relational\n",
    "for s1 in relat_index2:\n",
    "    if(\"multimodal\" in ref_mod[s1]):\n",
    "        ref_multi_rel=ref_multi_rel+1\n",
    "    \n",
    "\n",
    "for s2 in relat_index2:\n",
    "    if(\"speech only\" in ref_mod[s2]):\n",
    "        ref_speech_rel=ref_speech_rel+1\n",
    "    \n",
    "\n",
    "# pred multimodal -relational\n",
    "# pred speech only -relational  \n",
    "\n",
    "for p1 in relat_index1:\n",
    "    if(\"multimodal\" in pred_mod[p1]):\n",
    "        pred_multi_rel=pred_multi_rel+1\n",
    "    \n",
    "for p2 in relat_index1:\n",
    "    if(\"speech only\" in pred_mod[p2]):\n",
    "        pred_speech_rel=pred_speech_rel+1\n",
    "    \n",
    "        \n",
    "# ref multimodal - attributive\n",
    "# ref speech only - attributive\n",
    "\n",
    "for a1 in alt_index2:\n",
    "    if(\"multimodal\" in ref_mod[a1]):\n",
    "        ref_multi_att=ref_multi_att+1\n",
    "    \n",
    "\n",
    "for a2 in alt_index2:\n",
    "    if(\"speech only\" in ref_mod[a2]):\n",
    "        ref_speech_att=ref_speech_att+1\n",
    "    \n",
    "# pred multimodal -attributive\n",
    "# pred speech only -attributive      \n",
    "    \n",
    "for b1 in alt_index1:\n",
    "    if(\"multimodal\" in pred_mod[b1]):\n",
    "        pred_multi_att=pred_multi_att+1\n",
    "    \n",
    "\n",
    "for b2 in alt_index1:\n",
    "    if(\"speech only\" in pred_mod[b2]):\n",
    "        pred_speech_att=pred_speech_att+1\n",
    "\n",
    "        \n",
    "#   ......................................Position investigation........................................ \n",
    "\n",
    "for p1,p2 in zip(ref_pos,pred_pos):\n",
    "    if (p1 != p2): \n",
    "        incor_positions.append(\" REF: \"+ ref_speech[ref_pos.index(p1)] + \" , \" + ref_mod[ref_pos.index(p1)] + \" , \" + ref_pos[ref_pos.index(p1)] +\" PRED: \"+ pred_speech[ref_pos.index(p1)] + \" , \" + pred_mod[ref_pos.index(p1)] + \" , \" + pred_pos[ref_pos.index(p1)])                       \n",
    "    \n",
    "# sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "# sacrebleu_results=sacrebleu.compute(predictions=pred_speech, references=ref_speech)\n",
    "\n",
    "# rouge = evaluate.load('rouge')\n",
    "# rouge_results=rouge.compute(predictions=pred_speech, references=ref_speech)\n",
    "\n",
    "# METEOR = evaluate.load('meteor')\n",
    "# METEOR_results=METEOR.compute(predictions=pred_speech, references=ref_speech)\n",
    "\n",
    "\n",
    "bertscore_metric = load_metric('bertscore')\n",
    "bert_scores = bertscore_metric.compute(predictions=pred_speech, references=ref_speech, lang=\"en\")\n",
    "\n",
    "\n",
    "bert_scores2 = bertscore_metric.compute(predictions=pred_spch_spch, references=ref_spch_spch, lang=\"en\")\n",
    "\n",
    "# bleurt_metric = load_metric('bleurt')\n",
    "# bleurt_scores = bleurt_metric.compute(predictions=pred_speech, references=ref_speech)\n",
    "\n",
    "# accuracy_results=accuracy_score(predictions, references)\n",
    "# precision_results=precision_score(references, predictions,average = \"micro\",zero_division=1)\n",
    "# recall_results=recall_score(references, predictions,average =\"micro\",zero_division=1)\n",
    "# f1 =f1_score(references, predictions, average = \"micro\")\n",
    "\n",
    "sp_accuracy_results=accuracy_score(ref_speech, pred_speech)\n",
    "sp_precision_results=precision_score(ref_speech, pred_speech,average = \"micro\",zero_division=1)\n",
    "sp_recall_results=recall_score(ref_speech, pred_speech,average =\"micro\",zero_division=1)\n",
    "sp_f1 =f1_score(ref_speech, pred_speech, average = \"micro\")\n",
    "\n",
    "dem_accuracy_results=accuracy_score(ref_dem, pred_dem)\n",
    "dem_precision_results=precision_score(ref_dem, pred_dem,average = \"micro\",zero_division=1)\n",
    "dem_recall_results=recall_score(ref_dem, pred_dem,average =\"micro\",zero_division=1)\n",
    "dem_f1 =f1_score(ref_dem, pred_dem, average = \"micro\")\n",
    "\n",
    "mod_accuracy_results=accuracy_score(ref_mod, pred_mod)\n",
    "mod_precision_results=precision_score(ref_mod, pred_mod,average = \"micro\",zero_division=1)\n",
    "mod_recall_results=recall_score(ref_mod, pred_mod,average =\"micro\",zero_division=1)\n",
    "mod_f1 =f1_score(ref_mod, pred_mod, average = \"micro\")\n",
    "\n",
    "pos_accuracy_results=accuracy_score(ref_pos, pred_pos)\n",
    "pos_precision_results=precision_score(ref_pos, pred_pos,average = \"micro\",zero_division=0)\n",
    "pos_recall_results=recall_score(ref_pos, pred_pos,average =\"micro\",zero_division=1)\n",
    "pos_f1 =f1_score(ref_pos, pred_pos, average = \"micro\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"Number of reference pointing only REs: \", rpointing)\n",
    "print(\"Number of reference speech only REs: \", rspeech)\n",
    "print(\"Number of reference multimodal only REs: \", rmulti)\n",
    "print()\n",
    "print(\"Number of predicted pointing only REs: \", pointing)\n",
    "print(\"Number of predicted speech only REs: \", speech)\n",
    "print(\"Number of predicted multimodal only REs: \", multi)\n",
    "print()\n",
    "print(\"Number of identical pointing only REs for ref aand pred: \", pointing_pointing)\n",
    "print(\"Number of identical speech only REs for ref aand pred: \", speech_speech)\n",
    "print(\"Number of identical multimodal  REs for ref aand pred: \", multi_multi)\n",
    "print()\n",
    "print(\"Number of pointing speech REs for ref aand pred: \", pointing_speech)\n",
    "print(\"Number of speech_pointing REs for ref aand pred: \", speech_pointing)\n",
    "print()\n",
    "print(\"Number of multi_pointing REs for ref aand pred: \", multi_pointing)\n",
    "print(\"Number of pointing_multi REs for ref aand pred: \", pointing_multi)\n",
    "print()\n",
    "print(\"Number of speech_multi REs for ref aand pred: \", speech_multi)\n",
    "print(\"Number of multi_speech REs for ref aand pred: \", multi_speech)\n",
    "print()\n",
    "print(\"ref_multi_rel: \", ref_multi_rel)\n",
    "print(\"ref_speech_rel: \", ref_speech_rel)\n",
    "\n",
    "print(\"pred_multi_rel: \", pred_multi_rel)\n",
    "print(\"pred_speech_rel: \", pred_speech_rel)\n",
    "print()\n",
    "\n",
    "print(\"ref_multi_att: \", ref_multi_att)\n",
    "print(\"ref_speech_att: \", ref_speech_att)\n",
    "print()\n",
    "\n",
    "print(\"pred_multi_att: \", pred_multi_att)\n",
    "print(\"pred_speech_att: \", pred_speech_att)\n",
    "print()\n",
    "\n",
    "print()\n",
    "print(\"Evaluation Using Testing Data\")\n",
    "print()\n",
    "\n",
    "#N-gram based metrics have a critical disadvantage which only uses the sole characteristic of word matching. \n",
    "#In which, this cannot evaluate the true semantics of the whole sentence to have a fair comparison.\n",
    "print(\"N-gram based metrics\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print()\n",
    "\n",
    "# print(\"Bleu: \", sacrebleu_results[\"score\"])\n",
    "# print(\"Rouge: \", rouge_results)\n",
    "# print(\"METEOR: \", METEOR_results)\n",
    "print()\n",
    "# print(\"Tuple Accuracy: \", accuracy_results)\n",
    "# print(\"Tuple Precision: \", precision_results)\n",
    "# print(\"Tuple Recall: \", recall_results)\n",
    "# print(\"Tuple F1: \", f1)\n",
    "print()\n",
    "print(\"Speech Accuracy: \", sp_accuracy_results)\n",
    "print(\"Speech Precision: \", sp_precision_results)\n",
    "print(\"Speech Recall: \", sp_recall_results)\n",
    "print(\"Speech F1: \", sp_f1)\n",
    "print()\n",
    "print(\"Dem Accuracy: \", dem_accuracy_results)\n",
    "print(\"Dem Precision: \", dem_precision_results)\n",
    "print(\"Dem Recall: \", dem_recall_results)\n",
    "print(\"Dem F1: \", dem_f1)\n",
    "print()\n",
    "print(\"Mod Accuracy: \", mod_accuracy_results)\n",
    "print(\"Mod Precision: \", mod_precision_results)\n",
    "print(\"Mod Recall: \", mod_recall_results)\n",
    "print(\"Mod F1: \", mod_f1)\n",
    "print()\n",
    "print(\"Pos Accuracy: \", pos_accuracy_results)\n",
    "print(\"Pos Precision: \", pos_precision_results)\n",
    "print(\"Pos Recall: \", pos_recall_results)\n",
    "print(\"Pos F1: \", pos_f1)\n",
    "\n",
    "print(\"Incorrect postion predition details: \")\n",
    "for inc in incor_positions:\n",
    "    print(inc)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"MODEL BASED METRICS\")\n",
    "print(\"--------------------------------------------------------------------------------------------------\")\n",
    "print()\n",
    "#To retrieve the true semantics of a sentence, BERTScore leverages Transformer based model BERT embeddings. \n",
    "#After feeding the candidate and reference sentences through the model, \n",
    "#it calculates the similarity of the two sentences with cosine-similarity and uses importance weighting \n",
    "#to get the final scores.\n",
    "#PRECISION, RECALL, F1 for each data point\n",
    "#print(\"Bert_score (): \", bert_scores['precision'][0])\n",
    "\n",
    "print(\"Bert_score : \", bert_scores) \n",
    "\n",
    "total1=0\n",
    "for pre in bert_scores['precision']:\n",
    "    total1= total1 + pre\n",
    "average1=total1/testsamples\n",
    "print()\n",
    "print(\"All_Bert_precision average: \", average1)\n",
    "\n",
    "total1=0\n",
    "average1=0\n",
    "for re in bert_scores['recall']:\n",
    "    total1= total1 + re\n",
    "average1=total1/testsamples\n",
    "print()\n",
    "print(\"All_Bert_recall average: \", average1)\n",
    "\n",
    "total1=0\n",
    "average1=0\n",
    "for f1 in bert_scores['f1']:\n",
    "    total1= total1 + f1\n",
    "average1=total1/testsamples\n",
    "print()\n",
    "print(\"All_Bert_f1 average: \", average1)\n",
    "\n",
    "# ..............................................speech only.......................................................\n",
    "\n",
    "print(\"Bert_score : \", bert_scores2) \n",
    "total1=0\n",
    "for pre in bert_scores2['precision']:\n",
    "    total1= total1 + pre\n",
    "average1=total1/len(pred_spch_spch)\n",
    "print()\n",
    "print(\"speech only_Bert_precision average: \", average1)\n",
    "\n",
    "total1=0\n",
    "average1=0\n",
    "for re in bert_scores2['recall']:\n",
    "    total1= total1 + re\n",
    "average1=total1/len(pred_spch_spch)\n",
    "print()\n",
    "print(\"speech only_Bert_recall average: \", average1)\n",
    "\n",
    "total1=0\n",
    "average1=0\n",
    "for f1 in bert_scores2['f1']:\n",
    "    total1= total1 + f1\n",
    "average1=total1/len(pred_spch_spch)\n",
    "print()\n",
    "print(\"speech only_Bert_f1 average: \", average1)\n",
    "\n",
    "#BLEURT is a BERT-based model which is pre-trained on synthetic data and fine-tuned on a small rating dataset \n",
    "#to acquire human judgement knowledge\n",
    "print()\n",
    "# total2=0\n",
    "# print(\"Bleurt_score: \", bleurt_scores)\n",
    "# for bleu in bleurt_scores['scores']:\n",
    "#     total2= total2 + bleu    \n",
    "# average2=total2/testsamples\n",
    "# print()\n",
    "# print(\"Bleurt_score average: \", average2)\n",
    "\n",
    "# from datasets import list_metrics\n",
    "# metrics_list = list_metrics()\n",
    "# print(\"metrics_list:\" ,metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c7d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersection over Union (IoU) score\n",
    "def calculate_iou(sentence1, sentence2):\n",
    "    # Convert sentences to sets of words\n",
    "    set1 = set(sentence1.split())\n",
    "    set2 = set(sentence2.split())\n",
    "    \n",
    "    # Calculate intersection and union\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou = intersection / union if union != 0 else 0\n",
    "    \n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ebcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "total3=0\n",
    "with open('IoU-Orig-LORA8-13b-4batches-300epochs-q8-3e-4_synTesting.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['References', 'Predictions','IoU']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    for p,r in zip(predictions, references):\n",
    "        iou_score = calculate_iou(p, r)\n",
    "        writer.writerow({'References':r, 'Predictions':p,'IoU':iou_score})\n",
    "        total3=total3+iou_score\n",
    "\n",
    "iou_avg=total3/length\n",
    "print(\"Average of (IoU) score:\", iou_avg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c297195",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the model in gguf format either from HF or local\n",
    "from llama_cpp import Llama\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "# Huggingface\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Nadahass/MREG-13B-GGUF\", model_file=\"mreg-13b-32-hf.gguf\", model_type=\"llama\", gpu_layers=50)\n",
    "#local\n",
    "model = Llama(model_path=\"mreg-13b-32-hf.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a3b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =create_prompt(\"Generate a referring expression for an object.\",\"PinkBlock2 , (0.295275000: 1.224503000: 0.150793400) , None , ( 0.295275: 1.149781: 0.1507934) ) , None , None ,  put put grasp put put put put put put grasp put put put grasp put put put put , PinkBlock2 BlueBlock1 BlueBlock1 RedBlock2 BlueBlock2 YellowBlock1 YellowBlock2 YellowBlock1 RedBlock2 RedBlock2 RedBlock2 BlueBlock2 GreenBlock2 GreenBlock2 PinkBlock2 BlueBlock2 PinkBlock2 BlueBlock1\")\n",
    "print(model(prompt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
